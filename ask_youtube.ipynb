{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index\n",
    "# !pip install langchain\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 7274 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    ResponseSynthesizer,\n",
    "    download_loader\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "\n",
    "youtubelink = 'https://www.youtube.com/watch?v=FQBou-YgxyE'\n",
    "\n",
    "def open_file(filepath):\n",
    "    with open(filepath, 'r') as infile:\n",
    "        return infile.read()\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        query_engine = index.as_query_engine()\n",
    "        response = query_engine.query(user_input)\n",
    "        # response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n",
    "\n",
    "openai.api_key = open_file('./data/openaiapikey.txt')          \n",
    "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
    "\n",
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=[youtubelink])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 9 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2042 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  what is the advantage of using List index?\n",
      "Bot: \n",
      " The advantage of using a List index is that it allows you to explicitly go through every\n",
      "single piece of context within the index and use every single piece of context within the\n",
      "index to synthesize and answer for a given task. This is especially useful for\n",
      "summarization tasks, where you need to go through an entire document or set of documents\n",
      "to generate a summary. List index also allows you to store relationships between nodes,\n",
      "which can be used to generate more complex answers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 9 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1991 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  what is the disadvantage of using list index?\n",
      "Bot: \n",
      " One disadvantage of using list index is that it can be difficult to understand whether or\n",
      "not the knowledge has been included or not, especially for users without a lot of ML or\n",
      "deep learning experience. Additionally, it can be expensive if the data volumes are large.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2141 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  tell more about the Vector Store index,  pros and cons\n",
      "Bot: \n",
      " The Vector Store index is a data structure that allows for the storage of nodes and\n",
      "associated embeddings in a vector database. This index is useful for retrieving the most\n",
      "similar nodes to a query, as the query embedding can be used to retrieve the top K nodes\n",
      "from the vector database.   The pros of the Vector Store index include the ability to\n",
      "quickly retrieve the most similar nodes to a query, as well as the ability to store nodes\n",
      "and associated embeddings in a vector database. Additionally, the Vector Store index\n",
      "integrates with a variety of downstream vector databases, such as Chroma Pinecone, BVA\n",
      "Quadrant, and more.   The cons of the Vector Store index include the fact that it can be\n",
      "more expensive and have higher latency than other data structures, as it requires the\n",
      "retrieval of all nodes in the list. Additionally, the Vector Store index does not support\n",
      "multimodal documents, such as images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2002 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  which index can best work for summarization type of work?\n",
      "Bot: \n",
      " The List Index is best suited for summarization type of work. It allows for explicit\n",
      "traversal of the data and synthesis of a final answer. It can also be used to compose a\n",
      "graph of index structures over the data, allowing for more complex queries to be answered.\n",
      "Additionally, it can be used to convert natural language queries into SQL queries that can\n",
      "be executed against a SQL database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1749 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  how the multi-step query works \n",
      "Bot: \n",
      " Multi-step queries work by breaking a complex query into multiple simpler ones over the\n",
      "same data source. This allows the user to ask all the questions possible over the same\n",
      "data source to get back the answer. For example, if you wanted to know who was in the\n",
      "first batch of an accelerator program the author started, you could break this overall\n",
      "question down into simpler sub questions that you can answer sequentially until you have\n",
      "enough information to generate a final answer. The first question might be \"what\n",
      "accelerator program did the author start?\" and then you can ask \"who is in the first batch\n",
      "of that accelerator program?\" over the same data source. This effectively is similar to\n",
      "train of thought prompting but with the exception that it is specifically over your data\n",
      "source.\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import textwrap  # Import the textwrap module\n",
    "\n",
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(openai.api_key, index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    # print(f\"Bot: {response['content']}\")\n",
    "    # Wrap the text and print the wrapped content\n",
    "    wrapped_content = textwrap.wrap(response['content'], width=90)\n",
    "    print(\"User: \", user_input)\n",
    "    print(\"Bot: \",)\n",
    "    for line in wrapped_content:\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vendolly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
